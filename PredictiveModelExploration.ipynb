{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Goal**\n",
    "\n",
    "Using data about wells in Tanzania. The goal is to see if you can create a model to predict which water wells will be faulty, in order to go and fix them. The score will be based on 'accuracy'. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime as dt\n",
    "\n",
    "pd.set_option('display.max_columns', 500)\n",
    "y_train = pd.read_csv('/Users/lambda_school_loaner_95/Downloads/train_labels.csv')\n",
    "X_train = pd.read_csv('/Users/lambda_school_loaner_95/Downloads/train_features.csv')\n",
    "X_test = pd.read_csv('/Users/lambda_school_loaner_95/Downloads/test_features.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Quick Baseline**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(59400, 40)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "functional                 0.543081\n",
       "non functional             0.384242\n",
       "functional needs repair    0.072677\n",
       "Name: status_group, dtype: float64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train['status_group'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basic baseline to beat is 54% coming from the majority class in the target data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Beat the baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_target = y_train['status_group']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "functional                 0.543081\n",
       "non functional             0.384242\n",
       "functional needs repair    0.072677\n",
       "Name: status_group, dtype: float64"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train['status_group'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lambda_school_loaner_95/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:460: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n",
      "/Users/lambda_school_loaner_95/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:460: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n",
      "/Users/lambda_school_loaner_95/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:460: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n",
      "/Users/lambda_school_loaner_95/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:460: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n",
      "/Users/lambda_school_loaner_95/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:460: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.551801290559174"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "X_train_numbers = X_train[X_train.describe().columns]\n",
    "\n",
    "basic_model = LogisticRegression(solver='lbfgs', n_jobs=-1, random_state=42)\n",
    "cross_val_score(basic_model, X_train_numbers, y_train_target, scoring='accuracy', cv=5).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay... that did not do too well. But a slight improvement nonetheless. Moving on"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean data, feature engineer, test different models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## function that cleans the data, adds some features, ordinally encodes everything that is left\n",
    "\n",
    "import category_encoders as ce\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def wrangle_wells(X):\n",
    "    X = X.copy()\n",
    "    \n",
    "    # feature: get days passed since date recorded\n",
    "    X['last_day'] = '2014-01-01'\n",
    "    X['date_recorded'] = pd.to_datetime(X['date_recorded'], infer_datetime_format=True)\n",
    "    X['last_day'] = pd.to_datetime(X['last_day'], infer_datetime_format=True)\n",
    "    X['offset_days'] = X['last_day'] - X['date_recorded']\n",
    "    X['offset_days'] = X['offset_days'].dt.days\n",
    "    \n",
    "    # month date was recorded\n",
    "    X['month_recorded'] = X['date_recorded'].dt.month\n",
    "    \n",
    "    # years in service\n",
    "    X['construction_year'].fillna(X.groupby(['region', 'district_code'])['construction_year'].transform('median'), inplace=True)\n",
    "    X['construction_year'].fillna(X.groupby(['region'])['construction_year'].transform('median'), inplace=True)\n",
    "    X['construction_year'].fillna(X.groupby(['district_code'])['construction_year'].transform('median'), inplace=True)\n",
    "    X['construction_year'].fillna(X['construction_year'].median(), inplace=True)\n",
    "    \n",
    "    X['years_service'] = X.date_recorded.dt.year - X.construction_year\n",
    "    \n",
    "    # lower levels for funder categoricals\n",
    "    X.loc[X['funder'].isin((X['funder'].value_counts()[X['funder'].value_counts() < 850]).index), 'funder'] = 'other'\n",
    "    \n",
    "    # lower levels for installer\n",
    "    X.loc[X['installer'].isin((X['installer'].value_counts()[X['installer'].value_counts() < 620]).index), 'installer'] = 'other'\n",
    "    \n",
    "    # Try and make colum data a little more uniform\n",
    "    X.waterpoint_type = X.waterpoint_type.str.lower()\n",
    "    X.funder = X.funder.str.lower()\n",
    "    X.basin = X.basin.str.lower()\n",
    "    X.region = X.region.str.lower()\n",
    "    X.source = X.source.str.lower()\n",
    "    X.lga = X.lga.str.lower()\n",
    "    X.management = X.management.str.lower()\n",
    "    X.quantity = X.quantity.str.lower()\n",
    "    X.water_quality = X.water_quality.str.lower()\n",
    "    X.payment_type = X.payment_type.str.lower()\n",
    "    X.extraction_type = X.extraction_type.str.lower()\n",
    "    \n",
    "    # Lower cardinality of extraction type by adding to 'other' category\n",
    "    X['extraction_type'] = X['extraction_type'].replace({'other - mkulima/shinyanga': 'other'})\n",
    "    \n",
    "    \n",
    "    X = X.drop(columns=['recorded_by', 'quantity_group', 'date_recorded', 'wpt_name', 'num_private', 'subvillage',\n",
    "                       'region_code', 'management_group', 'extraction_type_group', 'extraction_type_class',\n",
    "                       'scheme_name', 'payment', 'water_quality', 'source_type', 'source_class', 'waterpoint_type_group',\n",
    "                       'ward', 'public_meeting', 'last_day', 'construction_year'])\n",
    "    \n",
    "    xyscaler = StandardScaler() \n",
    "    xyscaler.fit_transform(X[['latitude','longitude']])\n",
    "\n",
    "    X[\"rot45X\"] = .707* X['longitude'] + .707* X['latitude'] \n",
    "    X[\"rot45Y\"] = .707* X['longitude'] - .707* X['latitude']\n",
    "\n",
    "    X[\"rot30X\"] = (1.732/2)* X['latitude'] + (1./2)* X['longitude'] \n",
    "    X[\"rot30Y\"] = (1.732/2)* X['longitude'] - (1./2)* X['latitude']\n",
    "\n",
    "    X[\"rot60X\"] = (1./2)* X['latitude'] + (1.732/2)* X['longitude'] \n",
    "    X[\"rot60Y\"] = (1./2)* X['longitude'] - (1.732/2)* X['latitude']\n",
    "\n",
    "    X[\"radial_r\"] = np.sqrt( np.power(X['longitude'],2) + np.power(X['latitude'],2) )\n",
    "    \n",
    "    \n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## wrangle the test and train data\n",
    "\n",
    "X_test1 = wrangle_wells(X_train)\n",
    "X_test2 = wrangle_wells(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "## pre process train data to Ordinally Encode and then impute average values for all NaN's\n",
    "\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "features = ['id', 'amount_tsh', 'funder', 'gps_height',\n",
    "       'installer', 'longitude', 'latitude',\n",
    "       'basin', 'region', 'district_code', 'lga',\n",
    "       'population',\n",
    "       'scheme_management', 'permit',\n",
    "       'extraction_type',\n",
    "       'management', 'payment_type',\n",
    "       'quality_group', 'quantity',\n",
    "       'source', 'waterpoint_type',\n",
    "       'offset_days', 'month_recorded', 'years_service',\n",
    "       'rot45X', 'rot45Y', 'rot30X', 'rot30Y', 'rot60X', 'rot60Y', 'radial_r'\n",
    "   ]\n",
    "\n",
    "preprocessor = make_pipeline(ce.OrdinalEncoder(), SimpleImputer())\n",
    "X_test1 = preprocessor.fit_transform(X_test1)\n",
    "X_test1 = pd.DataFrame(X_test1, columns=features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform test data in the same way as the train data\n",
    "\n",
    "X_teste2 = preprocessor.transform(X_test2)\n",
    "X_teste2 = pd.DataFrame(X_teste2, columns=features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=1000, multi_class='warn',\n",
      "          n_jobs=None, penalty='l2', random_state=None, solver='lbfgs',\n",
      "          tol=0.0001, verbose=0, warm_start=False) \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lambda_school_loaner_95/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:460: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n",
      "/Users/lambda_school_loaner_95/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "/Users/lambda_school_loaner_95/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "/Users/lambda_school_loaner_95/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "/Users/lambda_school_loaner_95/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:460: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n",
      "/Users/lambda_school_loaner_95/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "/Users/lambda_school_loaner_95/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "/Users/lambda_school_loaner_95/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:460: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n",
      "/Users/lambda_school_loaner_95/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "/Users/lambda_school_loaner_95/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "/Users/lambda_school_loaner_95/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:460: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n",
      "/Users/lambda_school_loaner_95/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "/Users/lambda_school_loaner_95/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "/Users/lambda_school_loaner_95/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:460: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n",
      "/Users/lambda_school_loaner_95/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "/Users/lambda_school_loaner_95/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross_Validation Accuracy: 0.6063973696515584 \n",
      " \n",
      "\n",
      "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=3,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
      "            splitter='best') \n",
      "\n",
      "Cross_Validation Accuracy: 0.6934514989614184 \n",
      " \n",
      "\n",
      "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
      "            splitter='best') \n",
      "\n",
      "Cross_Validation Accuracy: 0.7466330313376597 \n",
      " \n",
      "\n",
      "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=3, max_features='auto', max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=50, n_jobs=-1,\n",
      "            oob_score=False, random_state=42, verbose=0, warm_start=False) \n",
      "\n",
      "Cross_Validation Accuracy: 0.6887042027413878 \n",
      " \n",
      "\n",
      "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=50, n_jobs=-1,\n",
      "            oob_score=False, random_state=42, verbose=0, warm_start=False) \n",
      "\n",
      "Cross_Validation Accuracy: 0.8069023510130691 \n",
      " \n",
      "\n",
      "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "       colsample_bytree=1, gamma=0, learning_rate=0.1, max_delta_step=0,\n",
      "       max_depth=3, min_child_weight=1, missing=None, n_estimators=50,\n",
      "       n_jobs=-1, nthread=None, objective='binary:logistic',\n",
      "       random_state=42, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
      "       seed=None, silent=True, subsample=1) \n",
      "\n",
      "Cross_Validation Accuracy: 0.7344784863650176 \n",
      " \n",
      "\n"
     ]
    }
   ],
   "source": [
    "## test different models to see which will work best\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "models = [LogisticRegression(solver='lbfgs', max_iter=1000),\n",
    "          DecisionTreeClassifier(max_depth=3),\n",
    "          DecisionTreeClassifier(max_depth=None),\n",
    "          RandomForestClassifier(max_depth=3, n_estimators=50, n_jobs=-1, random_state=42),\n",
    "          RandomForestClassifier(max_depth=None, n_estimators=50, n_jobs=-1, random_state=42),\n",
    "          XGBClassifier(max_depth=3, n_estimators=50, n_jobs=-1, random_state=42)]\n",
    "\n",
    "for model in models:\n",
    "  print(model, '\\n')\n",
    "  score = cross_val_score(model, X_test1, y_train_target, scoring='accuracy', cv=5).mean()\n",
    "  print('Cross_Validation Accuracy:', score, '\\n', '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At quick glance, it is nice to see that the data cleaning did help, even with the logistic regression model, the scores improved.  It is nice to see some significant jumps in scores with other models as well.  Even something as simple as a single decision tree can get the score up in the 70's.  Maybe due to some leakage somewhere.. maybe there are some highly predictive features in the model.  Not sure yet.  Either way, the Random Forest Classifier did the best, so that is what I will stick with"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model that Overfits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "param_distributions = {\n",
    "    'bootstrap': [True, False],\n",
    "    'max_depth': [10, 20, 30, 40, 50, 60, 70, 80, 90, 100, None],\n",
    "    'max_features': ['auto', 'sqrt'],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'n_estimators': [200, 400, 600, 800, 1000, 1200, 1400, 1600, 1800, 2000]\n",
    "}\n",
    "\n",
    "searchRF = RandomizedSearchCV(\n",
    "    estimator = RandomForestClassifier(n_jobs=-1, random_state=42),\n",
    "    param_distributions=param_distributions,\n",
    "    n_iter=50,\n",
    "    scoring='accuracy',\n",
    "    n_jobs=-1,\n",
    "    cv=5,\n",
    "    verbose=10,\n",
    "    return_train_score=True,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "searchRF.fit(X_test1, y_train_target)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_n_estimators</th>\n",
       "      <th>param_min_samples_split</th>\n",
       "      <th>param_min_samples_leaf</th>\n",
       "      <th>param_max_features</th>\n",
       "      <th>param_max_depth</th>\n",
       "      <th>param_bootstrap</th>\n",
       "      <th>params</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>split3_test_score</th>\n",
       "      <th>split4_test_score</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>rank_test_score</th>\n",
       "      <th>split0_train_score</th>\n",
       "      <th>split1_train_score</th>\n",
       "      <th>split2_train_score</th>\n",
       "      <th>split3_train_score</th>\n",
       "      <th>split4_train_score</th>\n",
       "      <th>mean_train_score</th>\n",
       "      <th>std_train_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>720.509379</td>\n",
       "      <td>5.798710</td>\n",
       "      <td>13.033700</td>\n",
       "      <td>0.503160</td>\n",
       "      <td>2000</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>auto</td>\n",
       "      <td>90</td>\n",
       "      <td>False</td>\n",
       "      <td>{'n_estimators': 2000, 'min_samples_split': 2,...</td>\n",
       "      <td>0.819880</td>\n",
       "      <td>0.811716</td>\n",
       "      <td>0.815152</td>\n",
       "      <td>0.817088</td>\n",
       "      <td>0.816720</td>\n",
       "      <td>0.816111</td>\n",
       "      <td>0.002675</td>\n",
       "      <td>1</td>\n",
       "      <td>0.951893</td>\n",
       "      <td>0.951598</td>\n",
       "      <td>0.952041</td>\n",
       "      <td>0.950526</td>\n",
       "      <td>0.951707</td>\n",
       "      <td>0.951553</td>\n",
       "      <td>0.000536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>729.220552</td>\n",
       "      <td>16.964503</td>\n",
       "      <td>12.409712</td>\n",
       "      <td>1.579188</td>\n",
       "      <td>2000</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>auto</td>\n",
       "      <td>70</td>\n",
       "      <td>False</td>\n",
       "      <td>{'n_estimators': 2000, 'min_samples_split': 2,...</td>\n",
       "      <td>0.819880</td>\n",
       "      <td>0.811716</td>\n",
       "      <td>0.815152</td>\n",
       "      <td>0.817088</td>\n",
       "      <td>0.816720</td>\n",
       "      <td>0.816111</td>\n",
       "      <td>0.002675</td>\n",
       "      <td>1</td>\n",
       "      <td>0.951893</td>\n",
       "      <td>0.951598</td>\n",
       "      <td>0.952041</td>\n",
       "      <td>0.950526</td>\n",
       "      <td>0.951707</td>\n",
       "      <td>0.951553</td>\n",
       "      <td>0.000536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>349.453606</td>\n",
       "      <td>1.907269</td>\n",
       "      <td>5.406091</td>\n",
       "      <td>0.230610</td>\n",
       "      <td>1000</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>sqrt</td>\n",
       "      <td>90</td>\n",
       "      <td>False</td>\n",
       "      <td>{'n_estimators': 1000, 'min_samples_split': 5,...</td>\n",
       "      <td>0.820638</td>\n",
       "      <td>0.811295</td>\n",
       "      <td>0.815067</td>\n",
       "      <td>0.816414</td>\n",
       "      <td>0.815794</td>\n",
       "      <td>0.815842</td>\n",
       "      <td>0.002987</td>\n",
       "      <td>3</td>\n",
       "      <td>0.951851</td>\n",
       "      <td>0.951619</td>\n",
       "      <td>0.951999</td>\n",
       "      <td>0.950442</td>\n",
       "      <td>0.951917</td>\n",
       "      <td>0.951566</td>\n",
       "      <td>0.000576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>5063.786933</td>\n",
       "      <td>2447.550383</td>\n",
       "      <td>3.359759</td>\n",
       "      <td>0.492994</td>\n",
       "      <td>600</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>auto</td>\n",
       "      <td>60</td>\n",
       "      <td>True</td>\n",
       "      <td>{'n_estimators': 600, 'min_samples_split': 5, ...</td>\n",
       "      <td>0.820554</td>\n",
       "      <td>0.813736</td>\n",
       "      <td>0.816835</td>\n",
       "      <td>0.813636</td>\n",
       "      <td>0.813605</td>\n",
       "      <td>0.815673</td>\n",
       "      <td>0.002733</td>\n",
       "      <td>4</td>\n",
       "      <td>0.981481</td>\n",
       "      <td>0.981902</td>\n",
       "      <td>0.981860</td>\n",
       "      <td>0.981566</td>\n",
       "      <td>0.981524</td>\n",
       "      <td>0.981667</td>\n",
       "      <td>0.000178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>408.932301</td>\n",
       "      <td>2.189365</td>\n",
       "      <td>10.959561</td>\n",
       "      <td>1.006485</td>\n",
       "      <td>1600</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>sqrt</td>\n",
       "      <td>50</td>\n",
       "      <td>True</td>\n",
       "      <td>{'n_estimators': 1600, 'min_samples_split': 5,...</td>\n",
       "      <td>0.821227</td>\n",
       "      <td>0.811800</td>\n",
       "      <td>0.815909</td>\n",
       "      <td>0.815152</td>\n",
       "      <td>0.814278</td>\n",
       "      <td>0.815673</td>\n",
       "      <td>0.003102</td>\n",
       "      <td>4</td>\n",
       "      <td>0.954671</td>\n",
       "      <td>0.954797</td>\n",
       "      <td>0.955471</td>\n",
       "      <td>0.954482</td>\n",
       "      <td>0.954926</td>\n",
       "      <td>0.954870</td>\n",
       "      <td>0.000335</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>493.976887</td>\n",
       "      <td>6.552700</td>\n",
       "      <td>7.843262</td>\n",
       "      <td>0.624889</td>\n",
       "      <td>1400</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>sqrt</td>\n",
       "      <td>30</td>\n",
       "      <td>False</td>\n",
       "      <td>{'n_estimators': 1400, 'min_samples_split': 5,...</td>\n",
       "      <td>0.820217</td>\n",
       "      <td>0.810959</td>\n",
       "      <td>0.816077</td>\n",
       "      <td>0.816498</td>\n",
       "      <td>0.814531</td>\n",
       "      <td>0.815657</td>\n",
       "      <td>0.003001</td>\n",
       "      <td>6</td>\n",
       "      <td>0.950946</td>\n",
       "      <td>0.950525</td>\n",
       "      <td>0.951157</td>\n",
       "      <td>0.950021</td>\n",
       "      <td>0.950928</td>\n",
       "      <td>0.950715</td>\n",
       "      <td>0.000403</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>592.136366</td>\n",
       "      <td>70.995140</td>\n",
       "      <td>16.395862</td>\n",
       "      <td>2.649887</td>\n",
       "      <td>2000</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>sqrt</td>\n",
       "      <td>70</td>\n",
       "      <td>True</td>\n",
       "      <td>{'n_estimators': 2000, 'min_samples_split': 5,...</td>\n",
       "      <td>0.820217</td>\n",
       "      <td>0.813147</td>\n",
       "      <td>0.816498</td>\n",
       "      <td>0.814731</td>\n",
       "      <td>0.813605</td>\n",
       "      <td>0.815640</td>\n",
       "      <td>0.002564</td>\n",
       "      <td>7</td>\n",
       "      <td>0.982070</td>\n",
       "      <td>0.982239</td>\n",
       "      <td>0.982029</td>\n",
       "      <td>0.981713</td>\n",
       "      <td>0.981924</td>\n",
       "      <td>0.981995</td>\n",
       "      <td>0.000174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>511.176914</td>\n",
       "      <td>5.466096</td>\n",
       "      <td>13.474008</td>\n",
       "      <td>1.147614</td>\n",
       "      <td>2000</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>sqrt</td>\n",
       "      <td>50</td>\n",
       "      <td>True</td>\n",
       "      <td>{'n_estimators': 2000, 'min_samples_split': 5,...</td>\n",
       "      <td>0.820217</td>\n",
       "      <td>0.813147</td>\n",
       "      <td>0.816498</td>\n",
       "      <td>0.814731</td>\n",
       "      <td>0.813605</td>\n",
       "      <td>0.815640</td>\n",
       "      <td>0.002564</td>\n",
       "      <td>7</td>\n",
       "      <td>0.982028</td>\n",
       "      <td>0.982239</td>\n",
       "      <td>0.982050</td>\n",
       "      <td>0.981734</td>\n",
       "      <td>0.981924</td>\n",
       "      <td>0.981995</td>\n",
       "      <td>0.000165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>452.435272</td>\n",
       "      <td>7.082760</td>\n",
       "      <td>10.639169</td>\n",
       "      <td>0.572026</td>\n",
       "      <td>1800</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>auto</td>\n",
       "      <td>30</td>\n",
       "      <td>True</td>\n",
       "      <td>{'n_estimators': 1800, 'min_samples_split': 5,...</td>\n",
       "      <td>0.821059</td>\n",
       "      <td>0.811885</td>\n",
       "      <td>0.815741</td>\n",
       "      <td>0.814815</td>\n",
       "      <td>0.814531</td>\n",
       "      <td>0.815606</td>\n",
       "      <td>0.003013</td>\n",
       "      <td>9</td>\n",
       "      <td>0.953976</td>\n",
       "      <td>0.953598</td>\n",
       "      <td>0.954524</td>\n",
       "      <td>0.953556</td>\n",
       "      <td>0.954105</td>\n",
       "      <td>0.953952</td>\n",
       "      <td>0.000356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>145.618699</td>\n",
       "      <td>0.554082</td>\n",
       "      <td>2.863658</td>\n",
       "      <td>0.061974</td>\n",
       "      <td>600</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>sqrt</td>\n",
       "      <td>None</td>\n",
       "      <td>True</td>\n",
       "      <td>{'n_estimators': 600, 'min_samples_split': 10,...</td>\n",
       "      <td>0.819712</td>\n",
       "      <td>0.813484</td>\n",
       "      <td>0.816667</td>\n",
       "      <td>0.814646</td>\n",
       "      <td>0.813184</td>\n",
       "      <td>0.815539</td>\n",
       "      <td>0.002419</td>\n",
       "      <td>10</td>\n",
       "      <td>0.935184</td>\n",
       "      <td>0.936299</td>\n",
       "      <td>0.935438</td>\n",
       "      <td>0.934933</td>\n",
       "      <td>0.935461</td>\n",
       "      <td>0.935463</td>\n",
       "      <td>0.000460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>730.862590</td>\n",
       "      <td>9.341759</td>\n",
       "      <td>12.571742</td>\n",
       "      <td>0.440390</td>\n",
       "      <td>2000</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>auto</td>\n",
       "      <td>90</td>\n",
       "      <td>False</td>\n",
       "      <td>{'n_estimators': 2000, 'min_samples_split': 10...</td>\n",
       "      <td>0.819880</td>\n",
       "      <td>0.812390</td>\n",
       "      <td>0.814899</td>\n",
       "      <td>0.814562</td>\n",
       "      <td>0.815962</td>\n",
       "      <td>0.815539</td>\n",
       "      <td>0.002462</td>\n",
       "      <td>10</td>\n",
       "      <td>0.965319</td>\n",
       "      <td>0.965530</td>\n",
       "      <td>0.966014</td>\n",
       "      <td>0.964141</td>\n",
       "      <td>0.965279</td>\n",
       "      <td>0.965257</td>\n",
       "      <td>0.000616</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>71.089106</td>\n",
       "      <td>0.828702</td>\n",
       "      <td>0.907931</td>\n",
       "      <td>0.032422</td>\n",
       "      <td>200</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>sqrt</td>\n",
       "      <td>30</td>\n",
       "      <td>False</td>\n",
       "      <td>{'n_estimators': 200, 'min_samples_split': 5, ...</td>\n",
       "      <td>0.821816</td>\n",
       "      <td>0.810875</td>\n",
       "      <td>0.814141</td>\n",
       "      <td>0.815825</td>\n",
       "      <td>0.814952</td>\n",
       "      <td>0.815522</td>\n",
       "      <td>0.003565</td>\n",
       "      <td>12</td>\n",
       "      <td>0.950525</td>\n",
       "      <td>0.950462</td>\n",
       "      <td>0.950758</td>\n",
       "      <td>0.949790</td>\n",
       "      <td>0.950528</td>\n",
       "      <td>0.950412</td>\n",
       "      <td>0.000327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>94.359531</td>\n",
       "      <td>0.736202</td>\n",
       "      <td>1.287312</td>\n",
       "      <td>0.110311</td>\n",
       "      <td>200</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>auto</td>\n",
       "      <td>30</td>\n",
       "      <td>False</td>\n",
       "      <td>{'n_estimators': 200, 'min_samples_split': 2, ...</td>\n",
       "      <td>0.821816</td>\n",
       "      <td>0.810875</td>\n",
       "      <td>0.814141</td>\n",
       "      <td>0.815825</td>\n",
       "      <td>0.814952</td>\n",
       "      <td>0.815522</td>\n",
       "      <td>0.003565</td>\n",
       "      <td>12</td>\n",
       "      <td>0.950525</td>\n",
       "      <td>0.950462</td>\n",
       "      <td>0.950758</td>\n",
       "      <td>0.949790</td>\n",
       "      <td>0.950528</td>\n",
       "      <td>0.950412</td>\n",
       "      <td>0.000327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>105.976748</td>\n",
       "      <td>7.346351</td>\n",
       "      <td>1.921987</td>\n",
       "      <td>0.130128</td>\n",
       "      <td>400</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>auto</td>\n",
       "      <td>100</td>\n",
       "      <td>True</td>\n",
       "      <td>{'n_estimators': 400, 'min_samples_split': 10,...</td>\n",
       "      <td>0.819460</td>\n",
       "      <td>0.813063</td>\n",
       "      <td>0.815488</td>\n",
       "      <td>0.815152</td>\n",
       "      <td>0.814278</td>\n",
       "      <td>0.815488</td>\n",
       "      <td>0.002156</td>\n",
       "      <td>14</td>\n",
       "      <td>0.935121</td>\n",
       "      <td>0.935815</td>\n",
       "      <td>0.935795</td>\n",
       "      <td>0.934470</td>\n",
       "      <td>0.935630</td>\n",
       "      <td>0.935366</td>\n",
       "      <td>0.000514</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>296.343008</td>\n",
       "      <td>1.363506</td>\n",
       "      <td>6.505740</td>\n",
       "      <td>0.440909</td>\n",
       "      <td>1200</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>sqrt</td>\n",
       "      <td>100</td>\n",
       "      <td>True</td>\n",
       "      <td>{'n_estimators': 1200, 'min_samples_split': 2,...</td>\n",
       "      <td>0.820470</td>\n",
       "      <td>0.811716</td>\n",
       "      <td>0.815236</td>\n",
       "      <td>0.814646</td>\n",
       "      <td>0.815289</td>\n",
       "      <td>0.815471</td>\n",
       "      <td>0.002823</td>\n",
       "      <td>15</td>\n",
       "      <td>0.963215</td>\n",
       "      <td>0.963720</td>\n",
       "      <td>0.964794</td>\n",
       "      <td>0.963131</td>\n",
       "      <td>0.962943</td>\n",
       "      <td>0.963561</td>\n",
       "      <td>0.000668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>381.245175</td>\n",
       "      <td>0.959058</td>\n",
       "      <td>11.912072</td>\n",
       "      <td>2.487934</td>\n",
       "      <td>1200</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>sqrt</td>\n",
       "      <td>60</td>\n",
       "      <td>True</td>\n",
       "      <td>{'n_estimators': 1200, 'min_samples_split': 2,...</td>\n",
       "      <td>0.820470</td>\n",
       "      <td>0.811716</td>\n",
       "      <td>0.815236</td>\n",
       "      <td>0.814646</td>\n",
       "      <td>0.815289</td>\n",
       "      <td>0.815471</td>\n",
       "      <td>0.002823</td>\n",
       "      <td>15</td>\n",
       "      <td>0.963215</td>\n",
       "      <td>0.963720</td>\n",
       "      <td>0.964794</td>\n",
       "      <td>0.963131</td>\n",
       "      <td>0.962943</td>\n",
       "      <td>0.963561</td>\n",
       "      <td>0.000668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>388.391566</td>\n",
       "      <td>43.832043</td>\n",
       "      <td>8.529679</td>\n",
       "      <td>1.078404</td>\n",
       "      <td>1400</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>auto</td>\n",
       "      <td>None</td>\n",
       "      <td>True</td>\n",
       "      <td>{'n_estimators': 1400, 'min_samples_split': 5,...</td>\n",
       "      <td>0.820638</td>\n",
       "      <td>0.811969</td>\n",
       "      <td>0.815320</td>\n",
       "      <td>0.814983</td>\n",
       "      <td>0.814278</td>\n",
       "      <td>0.815438</td>\n",
       "      <td>0.002851</td>\n",
       "      <td>17</td>\n",
       "      <td>0.954755</td>\n",
       "      <td>0.954608</td>\n",
       "      <td>0.955408</td>\n",
       "      <td>0.954377</td>\n",
       "      <td>0.955031</td>\n",
       "      <td>0.954836</td>\n",
       "      <td>0.000356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>253.138390</td>\n",
       "      <td>3.622253</td>\n",
       "      <td>5.038906</td>\n",
       "      <td>0.413614</td>\n",
       "      <td>1000</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>auto</td>\n",
       "      <td>50</td>\n",
       "      <td>True</td>\n",
       "      <td>{'n_estimators': 1000, 'min_samples_split': 10...</td>\n",
       "      <td>0.819207</td>\n",
       "      <td>0.812895</td>\n",
       "      <td>0.816582</td>\n",
       "      <td>0.814815</td>\n",
       "      <td>0.813521</td>\n",
       "      <td>0.815404</td>\n",
       "      <td>0.002282</td>\n",
       "      <td>18</td>\n",
       "      <td>0.934847</td>\n",
       "      <td>0.936552</td>\n",
       "      <td>0.935711</td>\n",
       "      <td>0.935101</td>\n",
       "      <td>0.935419</td>\n",
       "      <td>0.935526</td>\n",
       "      <td>0.000590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>700.544952</td>\n",
       "      <td>9.882919</td>\n",
       "      <td>12.649523</td>\n",
       "      <td>0.506787</td>\n",
       "      <td>1800</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>auto</td>\n",
       "      <td>80</td>\n",
       "      <td>False</td>\n",
       "      <td>{'n_estimators': 1800, 'min_samples_split': 10...</td>\n",
       "      <td>0.818450</td>\n",
       "      <td>0.812221</td>\n",
       "      <td>0.814815</td>\n",
       "      <td>0.815488</td>\n",
       "      <td>0.813016</td>\n",
       "      <td>0.814798</td>\n",
       "      <td>0.002174</td>\n",
       "      <td>19</td>\n",
       "      <td>0.983922</td>\n",
       "      <td>0.984680</td>\n",
       "      <td>0.984848</td>\n",
       "      <td>0.983817</td>\n",
       "      <td>0.984512</td>\n",
       "      <td>0.984356</td>\n",
       "      <td>0.000412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>680.305126</td>\n",
       "      <td>24.599462</td>\n",
       "      <td>12.119634</td>\n",
       "      <td>1.635553</td>\n",
       "      <td>1800</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>sqrt</td>\n",
       "      <td>20</td>\n",
       "      <td>False</td>\n",
       "      <td>{'n_estimators': 1800, 'min_samples_split': 5,...</td>\n",
       "      <td>0.819291</td>\n",
       "      <td>0.812390</td>\n",
       "      <td>0.816162</td>\n",
       "      <td>0.813721</td>\n",
       "      <td>0.811753</td>\n",
       "      <td>0.814663</td>\n",
       "      <td>0.002764</td>\n",
       "      <td>20</td>\n",
       "      <td>0.972137</td>\n",
       "      <td>0.972537</td>\n",
       "      <td>0.971907</td>\n",
       "      <td>0.971633</td>\n",
       "      <td>0.973276</td>\n",
       "      <td>0.972298</td>\n",
       "      <td>0.000572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>682.136706</td>\n",
       "      <td>25.348266</td>\n",
       "      <td>11.776721</td>\n",
       "      <td>1.152025</td>\n",
       "      <td>1800</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>auto</td>\n",
       "      <td>20</td>\n",
       "      <td>False</td>\n",
       "      <td>{'n_estimators': 1800, 'min_samples_split': 5,...</td>\n",
       "      <td>0.819291</td>\n",
       "      <td>0.812390</td>\n",
       "      <td>0.816162</td>\n",
       "      <td>0.813721</td>\n",
       "      <td>0.811753</td>\n",
       "      <td>0.814663</td>\n",
       "      <td>0.002764</td>\n",
       "      <td>20</td>\n",
       "      <td>0.972137</td>\n",
       "      <td>0.972537</td>\n",
       "      <td>0.971907</td>\n",
       "      <td>0.971633</td>\n",
       "      <td>0.973276</td>\n",
       "      <td>0.972298</td>\n",
       "      <td>0.000572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>1112.538896</td>\n",
       "      <td>1274.519431</td>\n",
       "      <td>14.684893</td>\n",
       "      <td>2.221643</td>\n",
       "      <td>1800</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>sqrt</td>\n",
       "      <td>50</td>\n",
       "      <td>True</td>\n",
       "      <td>{'n_estimators': 1800, 'min_samples_split': 2,...</td>\n",
       "      <td>0.819123</td>\n",
       "      <td>0.811632</td>\n",
       "      <td>0.814478</td>\n",
       "      <td>0.813973</td>\n",
       "      <td>0.813352</td>\n",
       "      <td>0.814512</td>\n",
       "      <td>0.002498</td>\n",
       "      <td>22</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>65.837109</td>\n",
       "      <td>5.867067</td>\n",
       "      <td>1.172589</td>\n",
       "      <td>0.193521</td>\n",
       "      <td>200</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>sqrt</td>\n",
       "      <td>20</td>\n",
       "      <td>True</td>\n",
       "      <td>{'n_estimators': 200, 'min_samples_split': 2, ...</td>\n",
       "      <td>0.819544</td>\n",
       "      <td>0.813568</td>\n",
       "      <td>0.813973</td>\n",
       "      <td>0.810606</td>\n",
       "      <td>0.811921</td>\n",
       "      <td>0.813923</td>\n",
       "      <td>0.003057</td>\n",
       "      <td>23</td>\n",
       "      <td>0.969065</td>\n",
       "      <td>0.968939</td>\n",
       "      <td>0.967172</td>\n",
       "      <td>0.968497</td>\n",
       "      <td>0.969277</td>\n",
       "      <td>0.968590</td>\n",
       "      <td>0.000754</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>104.894440</td>\n",
       "      <td>6.917692</td>\n",
       "      <td>1.925417</td>\n",
       "      <td>0.408957</td>\n",
       "      <td>400</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>auto</td>\n",
       "      <td>20</td>\n",
       "      <td>True</td>\n",
       "      <td>{'n_estimators': 400, 'min_samples_split': 2, ...</td>\n",
       "      <td>0.816514</td>\n",
       "      <td>0.813063</td>\n",
       "      <td>0.813721</td>\n",
       "      <td>0.812626</td>\n",
       "      <td>0.813100</td>\n",
       "      <td>0.813805</td>\n",
       "      <td>0.001399</td>\n",
       "      <td>24</td>\n",
       "      <td>0.931438</td>\n",
       "      <td>0.931627</td>\n",
       "      <td>0.930997</td>\n",
       "      <td>0.930787</td>\n",
       "      <td>0.932873</td>\n",
       "      <td>0.931545</td>\n",
       "      <td>0.000729</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>551.403973</td>\n",
       "      <td>146.087543</td>\n",
       "      <td>9.980960</td>\n",
       "      <td>4.734288</td>\n",
       "      <td>1800</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>auto</td>\n",
       "      <td>80</td>\n",
       "      <td>False</td>\n",
       "      <td>{'n_estimators': 1800, 'min_samples_split': 5,...</td>\n",
       "      <td>0.816935</td>\n",
       "      <td>0.810201</td>\n",
       "      <td>0.813215</td>\n",
       "      <td>0.813636</td>\n",
       "      <td>0.813100</td>\n",
       "      <td>0.813418</td>\n",
       "      <td>0.002140</td>\n",
       "      <td>25</td>\n",
       "      <td>0.994213</td>\n",
       "      <td>0.994423</td>\n",
       "      <td>0.994529</td>\n",
       "      <td>0.994423</td>\n",
       "      <td>0.994297</td>\n",
       "      <td>0.994377</td>\n",
       "      <td>0.000110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>610.815839</td>\n",
       "      <td>9.270596</td>\n",
       "      <td>12.973359</td>\n",
       "      <td>0.845996</td>\n",
       "      <td>1600</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>sqrt</td>\n",
       "      <td>80</td>\n",
       "      <td>False</td>\n",
       "      <td>{'n_estimators': 1600, 'min_samples_split': 5,...</td>\n",
       "      <td>0.816850</td>\n",
       "      <td>0.810369</td>\n",
       "      <td>0.813215</td>\n",
       "      <td>0.813384</td>\n",
       "      <td>0.813184</td>\n",
       "      <td>0.813401</td>\n",
       "      <td>0.002058</td>\n",
       "      <td>26</td>\n",
       "      <td>0.994150</td>\n",
       "      <td>0.994381</td>\n",
       "      <td>0.994592</td>\n",
       "      <td>0.994360</td>\n",
       "      <td>0.994297</td>\n",
       "      <td>0.994356</td>\n",
       "      <td>0.000143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>827.512180</td>\n",
       "      <td>51.275973</td>\n",
       "      <td>18.382350</td>\n",
       "      <td>2.061249</td>\n",
       "      <td>1600</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>sqrt</td>\n",
       "      <td>90</td>\n",
       "      <td>False</td>\n",
       "      <td>{'n_estimators': 1600, 'min_samples_split': 5,...</td>\n",
       "      <td>0.816850</td>\n",
       "      <td>0.810369</td>\n",
       "      <td>0.813215</td>\n",
       "      <td>0.813384</td>\n",
       "      <td>0.813184</td>\n",
       "      <td>0.813401</td>\n",
       "      <td>0.002058</td>\n",
       "      <td>26</td>\n",
       "      <td>0.994150</td>\n",
       "      <td>0.994381</td>\n",
       "      <td>0.994592</td>\n",
       "      <td>0.994360</td>\n",
       "      <td>0.994297</td>\n",
       "      <td>0.994356</td>\n",
       "      <td>0.000143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>52.588997</td>\n",
       "      <td>0.340683</td>\n",
       "      <td>1.027237</td>\n",
       "      <td>0.056823</td>\n",
       "      <td>200</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>auto</td>\n",
       "      <td>50</td>\n",
       "      <td>True</td>\n",
       "      <td>{'n_estimators': 200, 'min_samples_split': 2, ...</td>\n",
       "      <td>0.818534</td>\n",
       "      <td>0.810875</td>\n",
       "      <td>0.813384</td>\n",
       "      <td>0.811869</td>\n",
       "      <td>0.811163</td>\n",
       "      <td>0.813165</td>\n",
       "      <td>0.002822</td>\n",
       "      <td>28</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>445.006858</td>\n",
       "      <td>5.532842</td>\n",
       "      <td>9.274371</td>\n",
       "      <td>1.408735</td>\n",
       "      <td>1200</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>auto</td>\n",
       "      <td>70</td>\n",
       "      <td>False</td>\n",
       "      <td>{'n_estimators': 1200, 'min_samples_split': 2,...</td>\n",
       "      <td>0.817187</td>\n",
       "      <td>0.809275</td>\n",
       "      <td>0.813468</td>\n",
       "      <td>0.813552</td>\n",
       "      <td>0.811753</td>\n",
       "      <td>0.813047</td>\n",
       "      <td>0.002588</td>\n",
       "      <td>29</td>\n",
       "      <td>0.997538</td>\n",
       "      <td>0.997853</td>\n",
       "      <td>0.997811</td>\n",
       "      <td>0.997580</td>\n",
       "      <td>0.997601</td>\n",
       "      <td>0.997677</td>\n",
       "      <td>0.000129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>77.184565</td>\n",
       "      <td>4.014259</td>\n",
       "      <td>1.006591</td>\n",
       "      <td>0.046729</td>\n",
       "      <td>200</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>sqrt</td>\n",
       "      <td>40</td>\n",
       "      <td>False</td>\n",
       "      <td>{'n_estimators': 200, 'min_samples_split': 5, ...</td>\n",
       "      <td>0.816598</td>\n",
       "      <td>0.810454</td>\n",
       "      <td>0.811279</td>\n",
       "      <td>0.813805</td>\n",
       "      <td>0.812931</td>\n",
       "      <td>0.813013</td>\n",
       "      <td>0.002147</td>\n",
       "      <td>30</td>\n",
       "      <td>0.994150</td>\n",
       "      <td>0.994192</td>\n",
       "      <td>0.994360</td>\n",
       "      <td>0.994276</td>\n",
       "      <td>0.994045</td>\n",
       "      <td>0.994205</td>\n",
       "      <td>0.000108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>220.967531</td>\n",
       "      <td>1.620002</td>\n",
       "      <td>3.380624</td>\n",
       "      <td>0.068433</td>\n",
       "      <td>600</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>sqrt</td>\n",
       "      <td>50</td>\n",
       "      <td>False</td>\n",
       "      <td>{'n_estimators': 600, 'min_samples_split': 2, ...</td>\n",
       "      <td>0.817608</td>\n",
       "      <td>0.809107</td>\n",
       "      <td>0.812542</td>\n",
       "      <td>0.812795</td>\n",
       "      <td>0.812426</td>\n",
       "      <td>0.812896</td>\n",
       "      <td>0.002717</td>\n",
       "      <td>31</td>\n",
       "      <td>0.997622</td>\n",
       "      <td>0.997727</td>\n",
       "      <td>0.997896</td>\n",
       "      <td>0.997538</td>\n",
       "      <td>0.997622</td>\n",
       "      <td>0.997681</td>\n",
       "      <td>0.000123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>73.356926</td>\n",
       "      <td>0.466879</td>\n",
       "      <td>1.013600</td>\n",
       "      <td>0.053833</td>\n",
       "      <td>200</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>sqrt</td>\n",
       "      <td>50</td>\n",
       "      <td>False</td>\n",
       "      <td>{'n_estimators': 200, 'min_samples_split': 2, ...</td>\n",
       "      <td>0.817355</td>\n",
       "      <td>0.810622</td>\n",
       "      <td>0.812121</td>\n",
       "      <td>0.812205</td>\n",
       "      <td>0.811079</td>\n",
       "      <td>0.812677</td>\n",
       "      <td>0.002416</td>\n",
       "      <td>32</td>\n",
       "      <td>0.997601</td>\n",
       "      <td>0.997601</td>\n",
       "      <td>0.997811</td>\n",
       "      <td>0.997664</td>\n",
       "      <td>0.997433</td>\n",
       "      <td>0.997622</td>\n",
       "      <td>0.000122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>329.730996</td>\n",
       "      <td>10.687908</td>\n",
       "      <td>6.618619</td>\n",
       "      <td>0.486682</td>\n",
       "      <td>1000</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>auto</td>\n",
       "      <td>None</td>\n",
       "      <td>True</td>\n",
       "      <td>{'n_estimators': 1000, 'min_samples_split': 5,...</td>\n",
       "      <td>0.815925</td>\n",
       "      <td>0.809949</td>\n",
       "      <td>0.813636</td>\n",
       "      <td>0.811953</td>\n",
       "      <td>0.810406</td>\n",
       "      <td>0.812374</td>\n",
       "      <td>0.002197</td>\n",
       "      <td>33</td>\n",
       "      <td>0.908163</td>\n",
       "      <td>0.909173</td>\n",
       "      <td>0.908123</td>\n",
       "      <td>0.908628</td>\n",
       "      <td>0.908905</td>\n",
       "      <td>0.908598</td>\n",
       "      <td>0.000410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>148.392296</td>\n",
       "      <td>1.916126</td>\n",
       "      <td>2.166154</td>\n",
       "      <td>0.062569</td>\n",
       "      <td>400</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>sqrt</td>\n",
       "      <td>80</td>\n",
       "      <td>False</td>\n",
       "      <td>{'n_estimators': 400, 'min_samples_split': 2, ...</td>\n",
       "      <td>0.816261</td>\n",
       "      <td>0.809275</td>\n",
       "      <td>0.812205</td>\n",
       "      <td>0.812290</td>\n",
       "      <td>0.811500</td>\n",
       "      <td>0.812306</td>\n",
       "      <td>0.002258</td>\n",
       "      <td>34</td>\n",
       "      <td>0.997643</td>\n",
       "      <td>0.997790</td>\n",
       "      <td>0.997917</td>\n",
       "      <td>0.997601</td>\n",
       "      <td>0.997538</td>\n",
       "      <td>0.997698</td>\n",
       "      <td>0.000137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>127.287248</td>\n",
       "      <td>1.548906</td>\n",
       "      <td>2.565004</td>\n",
       "      <td>0.202288</td>\n",
       "      <td>400</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>sqrt</td>\n",
       "      <td>70</td>\n",
       "      <td>True</td>\n",
       "      <td>{'n_estimators': 400, 'min_samples_split': 2, ...</td>\n",
       "      <td>0.815672</td>\n",
       "      <td>0.808602</td>\n",
       "      <td>0.813131</td>\n",
       "      <td>0.811869</td>\n",
       "      <td>0.811163</td>\n",
       "      <td>0.812088</td>\n",
       "      <td>0.002323</td>\n",
       "      <td>35</td>\n",
       "      <td>0.908205</td>\n",
       "      <td>0.908458</td>\n",
       "      <td>0.908018</td>\n",
       "      <td>0.908586</td>\n",
       "      <td>0.908842</td>\n",
       "      <td>0.908422</td>\n",
       "      <td>0.000288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>283.505694</td>\n",
       "      <td>6.881977</td>\n",
       "      <td>5.466346</td>\n",
       "      <td>0.221133</td>\n",
       "      <td>1200</td>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "      <td>sqrt</td>\n",
       "      <td>100</td>\n",
       "      <td>True</td>\n",
       "      <td>{'n_estimators': 1200, 'min_samples_split': 10...</td>\n",
       "      <td>0.815420</td>\n",
       "      <td>0.809612</td>\n",
       "      <td>0.813384</td>\n",
       "      <td>0.811785</td>\n",
       "      <td>0.809816</td>\n",
       "      <td>0.812003</td>\n",
       "      <td>0.002197</td>\n",
       "      <td>36</td>\n",
       "      <td>0.901513</td>\n",
       "      <td>0.902123</td>\n",
       "      <td>0.902104</td>\n",
       "      <td>0.901747</td>\n",
       "      <td>0.902087</td>\n",
       "      <td>0.901915</td>\n",
       "      <td>0.000245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>298.373851</td>\n",
       "      <td>8.223683</td>\n",
       "      <td>5.832545</td>\n",
       "      <td>0.164158</td>\n",
       "      <td>1200</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>sqrt</td>\n",
       "      <td>20</td>\n",
       "      <td>True</td>\n",
       "      <td>{'n_estimators': 1200, 'min_samples_split': 10...</td>\n",
       "      <td>0.814325</td>\n",
       "      <td>0.810622</td>\n",
       "      <td>0.812879</td>\n",
       "      <td>0.810438</td>\n",
       "      <td>0.811500</td>\n",
       "      <td>0.811953</td>\n",
       "      <td>0.001467</td>\n",
       "      <td>37</td>\n",
       "      <td>0.899619</td>\n",
       "      <td>0.901576</td>\n",
       "      <td>0.900905</td>\n",
       "      <td>0.900715</td>\n",
       "      <td>0.901582</td>\n",
       "      <td>0.900880</td>\n",
       "      <td>0.000721</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>2298.658169</td>\n",
       "      <td>1564.542364</td>\n",
       "      <td>8.185188</td>\n",
       "      <td>0.225765</td>\n",
       "      <td>1600</td>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "      <td>sqrt</td>\n",
       "      <td>50</td>\n",
       "      <td>True</td>\n",
       "      <td>{'n_estimators': 1600, 'min_samples_split': 10...</td>\n",
       "      <td>0.815588</td>\n",
       "      <td>0.809612</td>\n",
       "      <td>0.812795</td>\n",
       "      <td>0.811616</td>\n",
       "      <td>0.809901</td>\n",
       "      <td>0.811902</td>\n",
       "      <td>0.002178</td>\n",
       "      <td>38</td>\n",
       "      <td>0.901534</td>\n",
       "      <td>0.902292</td>\n",
       "      <td>0.902378</td>\n",
       "      <td>0.902189</td>\n",
       "      <td>0.902298</td>\n",
       "      <td>0.902138</td>\n",
       "      <td>0.000308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>48.510454</td>\n",
       "      <td>3.025912</td>\n",
       "      <td>0.808255</td>\n",
       "      <td>0.038291</td>\n",
       "      <td>200</td>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "      <td>sqrt</td>\n",
       "      <td>90</td>\n",
       "      <td>True</td>\n",
       "      <td>{'n_estimators': 200, 'min_samples_split': 10,...</td>\n",
       "      <td>0.814073</td>\n",
       "      <td>0.808265</td>\n",
       "      <td>0.813300</td>\n",
       "      <td>0.812290</td>\n",
       "      <td>0.809901</td>\n",
       "      <td>0.811566</td>\n",
       "      <td>0.002167</td>\n",
       "      <td>39</td>\n",
       "      <td>0.901702</td>\n",
       "      <td>0.901934</td>\n",
       "      <td>0.901494</td>\n",
       "      <td>0.901747</td>\n",
       "      <td>0.901582</td>\n",
       "      <td>0.901692</td>\n",
       "      <td>0.000150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>50.583115</td>\n",
       "      <td>3.968498</td>\n",
       "      <td>0.882425</td>\n",
       "      <td>0.078363</td>\n",
       "      <td>200</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>auto</td>\n",
       "      <td>20</td>\n",
       "      <td>True</td>\n",
       "      <td>{'n_estimators': 200, 'min_samples_split': 2, ...</td>\n",
       "      <td>0.813400</td>\n",
       "      <td>0.807676</td>\n",
       "      <td>0.810606</td>\n",
       "      <td>0.808923</td>\n",
       "      <td>0.809648</td>\n",
       "      <td>0.810051</td>\n",
       "      <td>0.001929</td>\n",
       "      <td>40</td>\n",
       "      <td>0.892190</td>\n",
       "      <td>0.893706</td>\n",
       "      <td>0.891877</td>\n",
       "      <td>0.893834</td>\n",
       "      <td>0.894175</td>\n",
       "      <td>0.893157</td>\n",
       "      <td>0.000935</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>75.386482</td>\n",
       "      <td>0.686652</td>\n",
       "      <td>1.048126</td>\n",
       "      <td>0.041346</td>\n",
       "      <td>200</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>auto</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>{'n_estimators': 200, 'min_samples_split': 5, ...</td>\n",
       "      <td>0.815672</td>\n",
       "      <td>0.807844</td>\n",
       "      <td>0.807744</td>\n",
       "      <td>0.810606</td>\n",
       "      <td>0.807964</td>\n",
       "      <td>0.809966</td>\n",
       "      <td>0.003047</td>\n",
       "      <td>41</td>\n",
       "      <td>0.999684</td>\n",
       "      <td>0.999726</td>\n",
       "      <td>0.999790</td>\n",
       "      <td>0.999811</td>\n",
       "      <td>0.999642</td>\n",
       "      <td>0.999731</td>\n",
       "      <td>0.000063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>230.118033</td>\n",
       "      <td>9.642856</td>\n",
       "      <td>4.405551</td>\n",
       "      <td>0.257905</td>\n",
       "      <td>1000</td>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "      <td>sqrt</td>\n",
       "      <td>20</td>\n",
       "      <td>True</td>\n",
       "      <td>{'n_estimators': 1000, 'min_samples_split': 10...</td>\n",
       "      <td>0.812137</td>\n",
       "      <td>0.807592</td>\n",
       "      <td>0.811279</td>\n",
       "      <td>0.807828</td>\n",
       "      <td>0.809816</td>\n",
       "      <td>0.809731</td>\n",
       "      <td>0.001811</td>\n",
       "      <td>42</td>\n",
       "      <td>0.887982</td>\n",
       "      <td>0.888887</td>\n",
       "      <td>0.889057</td>\n",
       "      <td>0.889141</td>\n",
       "      <td>0.890303</td>\n",
       "      <td>0.889074</td>\n",
       "      <td>0.000741</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>149.722669</td>\n",
       "      <td>1.748836</td>\n",
       "      <td>2.303158</td>\n",
       "      <td>0.126286</td>\n",
       "      <td>400</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>sqrt</td>\n",
       "      <td>90</td>\n",
       "      <td>False</td>\n",
       "      <td>{'n_estimators': 400, 'min_samples_split': 2, ...</td>\n",
       "      <td>0.811380</td>\n",
       "      <td>0.803552</td>\n",
       "      <td>0.804545</td>\n",
       "      <td>0.806650</td>\n",
       "      <td>0.804176</td>\n",
       "      <td>0.806061</td>\n",
       "      <td>0.002856</td>\n",
       "      <td>43</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>331.014376</td>\n",
       "      <td>8.354716</td>\n",
       "      <td>6.730024</td>\n",
       "      <td>0.607305</td>\n",
       "      <td>800</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>sqrt</td>\n",
       "      <td>70</td>\n",
       "      <td>False</td>\n",
       "      <td>{'n_estimators': 800, 'min_samples_split': 2, ...</td>\n",
       "      <td>0.812137</td>\n",
       "      <td>0.802289</td>\n",
       "      <td>0.804125</td>\n",
       "      <td>0.806481</td>\n",
       "      <td>0.802576</td>\n",
       "      <td>0.805522</td>\n",
       "      <td>0.003627</td>\n",
       "      <td>44</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>108.119079</td>\n",
       "      <td>2.513103</td>\n",
       "      <td>1.596727</td>\n",
       "      <td>0.084284</td>\n",
       "      <td>200</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>sqrt</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>{'n_estimators': 200, 'min_samples_split': 2, ...</td>\n",
       "      <td>0.810875</td>\n",
       "      <td>0.804141</td>\n",
       "      <td>0.803704</td>\n",
       "      <td>0.805303</td>\n",
       "      <td>0.803418</td>\n",
       "      <td>0.805488</td>\n",
       "      <td>0.002769</td>\n",
       "      <td>45</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>180.368051</td>\n",
       "      <td>0.607344</td>\n",
       "      <td>2.322978</td>\n",
       "      <td>0.032859</td>\n",
       "      <td>800</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>auto</td>\n",
       "      <td>10</td>\n",
       "      <td>False</td>\n",
       "      <td>{'n_estimators': 800, 'min_samples_split': 10,...</td>\n",
       "      <td>0.766434</td>\n",
       "      <td>0.760290</td>\n",
       "      <td>0.767593</td>\n",
       "      <td>0.760690</td>\n",
       "      <td>0.772942</td>\n",
       "      <td>0.765589</td>\n",
       "      <td>0.004709</td>\n",
       "      <td>46</td>\n",
       "      <td>0.784697</td>\n",
       "      <td>0.785707</td>\n",
       "      <td>0.787016</td>\n",
       "      <td>0.783880</td>\n",
       "      <td>0.785762</td>\n",
       "      <td>0.785412</td>\n",
       "      <td>0.001062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>460.886029</td>\n",
       "      <td>4.623063</td>\n",
       "      <td>6.019484</td>\n",
       "      <td>0.256234</td>\n",
       "      <td>2000</td>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "      <td>sqrt</td>\n",
       "      <td>10</td>\n",
       "      <td>False</td>\n",
       "      <td>{'n_estimators': 2000, 'min_samples_split': 10...</td>\n",
       "      <td>0.765171</td>\n",
       "      <td>0.759700</td>\n",
       "      <td>0.767424</td>\n",
       "      <td>0.761616</td>\n",
       "      <td>0.771931</td>\n",
       "      <td>0.765168</td>\n",
       "      <td>0.004321</td>\n",
       "      <td>47</td>\n",
       "      <td>0.782550</td>\n",
       "      <td>0.783729</td>\n",
       "      <td>0.784870</td>\n",
       "      <td>0.782681</td>\n",
       "      <td>0.783805</td>\n",
       "      <td>0.783527</td>\n",
       "      <td>0.000847</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>159.322675</td>\n",
       "      <td>1.912035</td>\n",
       "      <td>2.860052</td>\n",
       "      <td>0.045670</td>\n",
       "      <td>1000</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>auto</td>\n",
       "      <td>10</td>\n",
       "      <td>True</td>\n",
       "      <td>{'n_estimators': 1000, 'min_samples_split': 5,...</td>\n",
       "      <td>0.762815</td>\n",
       "      <td>0.759869</td>\n",
       "      <td>0.766751</td>\n",
       "      <td>0.759764</td>\n",
       "      <td>0.770163</td>\n",
       "      <td>0.763872</td>\n",
       "      <td>0.004047</td>\n",
       "      <td>48</td>\n",
       "      <td>0.781224</td>\n",
       "      <td>0.783371</td>\n",
       "      <td>0.784638</td>\n",
       "      <td>0.781019</td>\n",
       "      <td>0.783406</td>\n",
       "      <td>0.782731</td>\n",
       "      <td>0.001393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>32.870517</td>\n",
       "      <td>0.695827</td>\n",
       "      <td>0.461440</td>\n",
       "      <td>0.039711</td>\n",
       "      <td>200</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>auto</td>\n",
       "      <td>10</td>\n",
       "      <td>True</td>\n",
       "      <td>{'n_estimators': 200, 'min_samples_split': 5, ...</td>\n",
       "      <td>0.763235</td>\n",
       "      <td>0.758859</td>\n",
       "      <td>0.764394</td>\n",
       "      <td>0.760606</td>\n",
       "      <td>0.769406</td>\n",
       "      <td>0.763300</td>\n",
       "      <td>0.003618</td>\n",
       "      <td>49</td>\n",
       "      <td>0.778425</td>\n",
       "      <td>0.780130</td>\n",
       "      <td>0.780703</td>\n",
       "      <td>0.778914</td>\n",
       "      <td>0.780754</td>\n",
       "      <td>0.779785</td>\n",
       "      <td>0.000949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>427.974828</td>\n",
       "      <td>2.331909</td>\n",
       "      <td>8.903878</td>\n",
       "      <td>1.101831</td>\n",
       "      <td>2000</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>sqrt</td>\n",
       "      <td>10</td>\n",
       "      <td>True</td>\n",
       "      <td>{'n_estimators': 2000, 'min_samples_split': 10...</td>\n",
       "      <td>0.762730</td>\n",
       "      <td>0.757596</td>\n",
       "      <td>0.765909</td>\n",
       "      <td>0.758754</td>\n",
       "      <td>0.769995</td>\n",
       "      <td>0.762997</td>\n",
       "      <td>0.004576</td>\n",
       "      <td>50</td>\n",
       "      <td>0.779036</td>\n",
       "      <td>0.779835</td>\n",
       "      <td>0.781271</td>\n",
       "      <td>0.779566</td>\n",
       "      <td>0.780312</td>\n",
       "      <td>0.780004</td>\n",
       "      <td>0.000756</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    mean_fit_time  std_fit_time  mean_score_time  std_score_time  \\\n",
       "11     720.509379      5.798710        13.033700        0.503160   \n",
       "33     729.220552     16.964503        12.409712        1.579188   \n",
       "9      349.453606      1.907269         5.406091        0.230610   \n",
       "42    5063.786933   2447.550383         3.359759        0.492994   \n",
       "5      408.932301      2.189365        10.959561        1.006485   \n",
       "18     493.976887      6.552700         7.843262        0.624889   \n",
       "19     592.136366     70.995140        16.395862        2.649887   \n",
       "48     511.176914      5.466096        13.474008        1.147614   \n",
       "7      452.435272      7.082760        10.639169        0.572026   \n",
       "36     145.618699      0.554082         2.863658        0.061974   \n",
       "12     730.862590      9.341759        12.571742        0.440390   \n",
       "37      71.089106      0.828702         0.907931        0.032422   \n",
       "21      94.359531      0.736202         1.287312        0.110311   \n",
       "32     105.976748      7.346351         1.921987        0.130128   \n",
       "28     296.343008      1.363506         6.505740        0.440909   \n",
       "1      381.245175      0.959058        11.912072        2.487934   \n",
       "2      388.391566     43.832043         8.529679        1.078404   \n",
       "46     253.138390      3.622253         5.038906        0.413614   \n",
       "10     700.544952      9.882919        12.649523        0.506787   \n",
       "38     680.305126     24.599462        12.119634        1.635553   \n",
       "26     682.136706     25.348266        11.776721        1.152025   \n",
       "44    1112.538896   1274.519431        14.684893        2.221643   \n",
       "25      65.837109      5.867067         1.172589        0.193521   \n",
       "30     104.894440      6.917692         1.925417        0.408957   \n",
       "49     551.403973    146.087543         9.980960        4.734288   \n",
       "31     610.815839      9.270596        12.973359        0.845996   \n",
       "23     827.512180     51.275973        18.382350        2.061249   \n",
       "3       52.588997      0.340683         1.027237        0.056823   \n",
       "17     445.006858      5.532842         9.274371        1.408735   \n",
       "16      77.184565      4.014259         1.006591        0.046729   \n",
       "8      220.967531      1.620002         3.380624        0.068433   \n",
       "35      73.356926      0.466879         1.013600        0.053833   \n",
       "20     329.730996     10.687908         6.618619        0.486682   \n",
       "39     148.392296      1.916126         2.166154        0.062569   \n",
       "22     127.287248      1.548906         2.565004        0.202288   \n",
       "29     283.505694      6.881977         5.466346        0.221133   \n",
       "4      298.373851      8.223683         5.832545        0.164158   \n",
       "45    2298.658169   1564.542364         8.185188        0.225765   \n",
       "27      48.510454      3.025912         0.808255        0.038291   \n",
       "43      50.583115      3.968498         0.882425        0.078363   \n",
       "41      75.386482      0.686652         1.048126        0.041346   \n",
       "13     230.118033      9.642856         4.405551        0.257905   \n",
       "14     149.722669      1.748836         2.303158        0.126286   \n",
       "6      331.014376      8.354716         6.730024        0.607305   \n",
       "24     108.119079      2.513103         1.596727        0.084284   \n",
       "40     180.368051      0.607344         2.322978        0.032859   \n",
       "15     460.886029      4.623063         6.019484        0.256234   \n",
       "34     159.322675      1.912035         2.860052        0.045670   \n",
       "47      32.870517      0.695827         0.461440        0.039711   \n",
       "0      427.974828      2.331909         8.903878        1.101831   \n",
       "\n",
       "   param_n_estimators param_min_samples_split param_min_samples_leaf  \\\n",
       "11               2000                       2                      4   \n",
       "33               2000                       2                      4   \n",
       "9                1000                       5                      4   \n",
       "42                600                       5                      1   \n",
       "5                1600                       5                      2   \n",
       "18               1400                       5                      4   \n",
       "19               2000                       5                      1   \n",
       "48               2000                       5                      1   \n",
       "7                1800                       5                      2   \n",
       "36                600                      10                      1   \n",
       "12               2000                      10                      2   \n",
       "37                200                       5                      4   \n",
       "21                200                       2                      4   \n",
       "32                400                      10                      1   \n",
       "28               1200                       2                      2   \n",
       "1                1200                       2                      2   \n",
       "2                1400                       5                      2   \n",
       "46               1000                      10                      1   \n",
       "10               1800                      10                      1   \n",
       "38               1800                       5                      1   \n",
       "26               1800                       5                      1   \n",
       "44               1800                       2                      1   \n",
       "25                200                       2                      1   \n",
       "30                400                       2                      2   \n",
       "49               1800                       5                      2   \n",
       "31               1600                       5                      2   \n",
       "23               1600                       5                      2   \n",
       "3                 200                       2                      1   \n",
       "17               1200                       2                      2   \n",
       "16                200                       5                      2   \n",
       "8                 600                       2                      2   \n",
       "35                200                       2                      2   \n",
       "20               1000                       5                      4   \n",
       "39                400                       2                      2   \n",
       "22                400                       2                      4   \n",
       "29               1200                      10                      4   \n",
       "4                1200                      10                      2   \n",
       "45               1600                      10                      4   \n",
       "27                200                      10                      4   \n",
       "43                200                       2                      4   \n",
       "41                200                       5                      1   \n",
       "13               1000                      10                      4   \n",
       "14                400                       2                      1   \n",
       "6                 800                       2                      1   \n",
       "24                200                       2                      1   \n",
       "40                800                      10                      1   \n",
       "15               2000                      10                      4   \n",
       "34               1000                       5                      1   \n",
       "47                200                       5                      4   \n",
       "0                2000                      10                      2   \n",
       "\n",
       "   param_max_features param_max_depth param_bootstrap  \\\n",
       "11               auto              90           False   \n",
       "33               auto              70           False   \n",
       "9                sqrt              90           False   \n",
       "42               auto              60            True   \n",
       "5                sqrt              50            True   \n",
       "18               sqrt              30           False   \n",
       "19               sqrt              70            True   \n",
       "48               sqrt              50            True   \n",
       "7                auto              30            True   \n",
       "36               sqrt            None            True   \n",
       "12               auto              90           False   \n",
       "37               sqrt              30           False   \n",
       "21               auto              30           False   \n",
       "32               auto             100            True   \n",
       "28               sqrt             100            True   \n",
       "1                sqrt              60            True   \n",
       "2                auto            None            True   \n",
       "46               auto              50            True   \n",
       "10               auto              80           False   \n",
       "38               sqrt              20           False   \n",
       "26               auto              20           False   \n",
       "44               sqrt              50            True   \n",
       "25               sqrt              20            True   \n",
       "30               auto              20            True   \n",
       "49               auto              80           False   \n",
       "31               sqrt              80           False   \n",
       "23               sqrt              90           False   \n",
       "3                auto              50            True   \n",
       "17               auto              70           False   \n",
       "16               sqrt              40           False   \n",
       "8                sqrt              50           False   \n",
       "35               sqrt              50           False   \n",
       "20               auto            None            True   \n",
       "39               sqrt              80           False   \n",
       "22               sqrt              70            True   \n",
       "29               sqrt             100            True   \n",
       "4                sqrt              20            True   \n",
       "45               sqrt              50            True   \n",
       "27               sqrt              90            True   \n",
       "43               auto              20            True   \n",
       "41               auto            None           False   \n",
       "13               sqrt              20            True   \n",
       "14               sqrt              90           False   \n",
       "6                sqrt              70           False   \n",
       "24               sqrt            None           False   \n",
       "40               auto              10           False   \n",
       "15               sqrt              10           False   \n",
       "34               auto              10            True   \n",
       "47               auto              10            True   \n",
       "0                sqrt              10            True   \n",
       "\n",
       "                                               params  split0_test_score  \\\n",
       "11  {'n_estimators': 2000, 'min_samples_split': 2,...           0.819880   \n",
       "33  {'n_estimators': 2000, 'min_samples_split': 2,...           0.819880   \n",
       "9   {'n_estimators': 1000, 'min_samples_split': 5,...           0.820638   \n",
       "42  {'n_estimators': 600, 'min_samples_split': 5, ...           0.820554   \n",
       "5   {'n_estimators': 1600, 'min_samples_split': 5,...           0.821227   \n",
       "18  {'n_estimators': 1400, 'min_samples_split': 5,...           0.820217   \n",
       "19  {'n_estimators': 2000, 'min_samples_split': 5,...           0.820217   \n",
       "48  {'n_estimators': 2000, 'min_samples_split': 5,...           0.820217   \n",
       "7   {'n_estimators': 1800, 'min_samples_split': 5,...           0.821059   \n",
       "36  {'n_estimators': 600, 'min_samples_split': 10,...           0.819712   \n",
       "12  {'n_estimators': 2000, 'min_samples_split': 10...           0.819880   \n",
       "37  {'n_estimators': 200, 'min_samples_split': 5, ...           0.821816   \n",
       "21  {'n_estimators': 200, 'min_samples_split': 2, ...           0.821816   \n",
       "32  {'n_estimators': 400, 'min_samples_split': 10,...           0.819460   \n",
       "28  {'n_estimators': 1200, 'min_samples_split': 2,...           0.820470   \n",
       "1   {'n_estimators': 1200, 'min_samples_split': 2,...           0.820470   \n",
       "2   {'n_estimators': 1400, 'min_samples_split': 5,...           0.820638   \n",
       "46  {'n_estimators': 1000, 'min_samples_split': 10...           0.819207   \n",
       "10  {'n_estimators': 1800, 'min_samples_split': 10...           0.818450   \n",
       "38  {'n_estimators': 1800, 'min_samples_split': 5,...           0.819291   \n",
       "26  {'n_estimators': 1800, 'min_samples_split': 5,...           0.819291   \n",
       "44  {'n_estimators': 1800, 'min_samples_split': 2,...           0.819123   \n",
       "25  {'n_estimators': 200, 'min_samples_split': 2, ...           0.819544   \n",
       "30  {'n_estimators': 400, 'min_samples_split': 2, ...           0.816514   \n",
       "49  {'n_estimators': 1800, 'min_samples_split': 5,...           0.816935   \n",
       "31  {'n_estimators': 1600, 'min_samples_split': 5,...           0.816850   \n",
       "23  {'n_estimators': 1600, 'min_samples_split': 5,...           0.816850   \n",
       "3   {'n_estimators': 200, 'min_samples_split': 2, ...           0.818534   \n",
       "17  {'n_estimators': 1200, 'min_samples_split': 2,...           0.817187   \n",
       "16  {'n_estimators': 200, 'min_samples_split': 5, ...           0.816598   \n",
       "8   {'n_estimators': 600, 'min_samples_split': 2, ...           0.817608   \n",
       "35  {'n_estimators': 200, 'min_samples_split': 2, ...           0.817355   \n",
       "20  {'n_estimators': 1000, 'min_samples_split': 5,...           0.815925   \n",
       "39  {'n_estimators': 400, 'min_samples_split': 2, ...           0.816261   \n",
       "22  {'n_estimators': 400, 'min_samples_split': 2, ...           0.815672   \n",
       "29  {'n_estimators': 1200, 'min_samples_split': 10...           0.815420   \n",
       "4   {'n_estimators': 1200, 'min_samples_split': 10...           0.814325   \n",
       "45  {'n_estimators': 1600, 'min_samples_split': 10...           0.815588   \n",
       "27  {'n_estimators': 200, 'min_samples_split': 10,...           0.814073   \n",
       "43  {'n_estimators': 200, 'min_samples_split': 2, ...           0.813400   \n",
       "41  {'n_estimators': 200, 'min_samples_split': 5, ...           0.815672   \n",
       "13  {'n_estimators': 1000, 'min_samples_split': 10...           0.812137   \n",
       "14  {'n_estimators': 400, 'min_samples_split': 2, ...           0.811380   \n",
       "6   {'n_estimators': 800, 'min_samples_split': 2, ...           0.812137   \n",
       "24  {'n_estimators': 200, 'min_samples_split': 2, ...           0.810875   \n",
       "40  {'n_estimators': 800, 'min_samples_split': 10,...           0.766434   \n",
       "15  {'n_estimators': 2000, 'min_samples_split': 10...           0.765171   \n",
       "34  {'n_estimators': 1000, 'min_samples_split': 5,...           0.762815   \n",
       "47  {'n_estimators': 200, 'min_samples_split': 5, ...           0.763235   \n",
       "0   {'n_estimators': 2000, 'min_samples_split': 10...           0.762730   \n",
       "\n",
       "    split1_test_score  split2_test_score  split3_test_score  \\\n",
       "11           0.811716           0.815152           0.817088   \n",
       "33           0.811716           0.815152           0.817088   \n",
       "9            0.811295           0.815067           0.816414   \n",
       "42           0.813736           0.816835           0.813636   \n",
       "5            0.811800           0.815909           0.815152   \n",
       "18           0.810959           0.816077           0.816498   \n",
       "19           0.813147           0.816498           0.814731   \n",
       "48           0.813147           0.816498           0.814731   \n",
       "7            0.811885           0.815741           0.814815   \n",
       "36           0.813484           0.816667           0.814646   \n",
       "12           0.812390           0.814899           0.814562   \n",
       "37           0.810875           0.814141           0.815825   \n",
       "21           0.810875           0.814141           0.815825   \n",
       "32           0.813063           0.815488           0.815152   \n",
       "28           0.811716           0.815236           0.814646   \n",
       "1            0.811716           0.815236           0.814646   \n",
       "2            0.811969           0.815320           0.814983   \n",
       "46           0.812895           0.816582           0.814815   \n",
       "10           0.812221           0.814815           0.815488   \n",
       "38           0.812390           0.816162           0.813721   \n",
       "26           0.812390           0.816162           0.813721   \n",
       "44           0.811632           0.814478           0.813973   \n",
       "25           0.813568           0.813973           0.810606   \n",
       "30           0.813063           0.813721           0.812626   \n",
       "49           0.810201           0.813215           0.813636   \n",
       "31           0.810369           0.813215           0.813384   \n",
       "23           0.810369           0.813215           0.813384   \n",
       "3            0.810875           0.813384           0.811869   \n",
       "17           0.809275           0.813468           0.813552   \n",
       "16           0.810454           0.811279           0.813805   \n",
       "8            0.809107           0.812542           0.812795   \n",
       "35           0.810622           0.812121           0.812205   \n",
       "20           0.809949           0.813636           0.811953   \n",
       "39           0.809275           0.812205           0.812290   \n",
       "22           0.808602           0.813131           0.811869   \n",
       "29           0.809612           0.813384           0.811785   \n",
       "4            0.810622           0.812879           0.810438   \n",
       "45           0.809612           0.812795           0.811616   \n",
       "27           0.808265           0.813300           0.812290   \n",
       "43           0.807676           0.810606           0.808923   \n",
       "41           0.807844           0.807744           0.810606   \n",
       "13           0.807592           0.811279           0.807828   \n",
       "14           0.803552           0.804545           0.806650   \n",
       "6            0.802289           0.804125           0.806481   \n",
       "24           0.804141           0.803704           0.805303   \n",
       "40           0.760290           0.767593           0.760690   \n",
       "15           0.759700           0.767424           0.761616   \n",
       "34           0.759869           0.766751           0.759764   \n",
       "47           0.758859           0.764394           0.760606   \n",
       "0            0.757596           0.765909           0.758754   \n",
       "\n",
       "    split4_test_score  mean_test_score  std_test_score  rank_test_score  \\\n",
       "11           0.816720         0.816111        0.002675                1   \n",
       "33           0.816720         0.816111        0.002675                1   \n",
       "9            0.815794         0.815842        0.002987                3   \n",
       "42           0.813605         0.815673        0.002733                4   \n",
       "5            0.814278         0.815673        0.003102                4   \n",
       "18           0.814531         0.815657        0.003001                6   \n",
       "19           0.813605         0.815640        0.002564                7   \n",
       "48           0.813605         0.815640        0.002564                7   \n",
       "7            0.814531         0.815606        0.003013                9   \n",
       "36           0.813184         0.815539        0.002419               10   \n",
       "12           0.815962         0.815539        0.002462               10   \n",
       "37           0.814952         0.815522        0.003565               12   \n",
       "21           0.814952         0.815522        0.003565               12   \n",
       "32           0.814278         0.815488        0.002156               14   \n",
       "28           0.815289         0.815471        0.002823               15   \n",
       "1            0.815289         0.815471        0.002823               15   \n",
       "2            0.814278         0.815438        0.002851               17   \n",
       "46           0.813521         0.815404        0.002282               18   \n",
       "10           0.813016         0.814798        0.002174               19   \n",
       "38           0.811753         0.814663        0.002764               20   \n",
       "26           0.811753         0.814663        0.002764               20   \n",
       "44           0.813352         0.814512        0.002498               22   \n",
       "25           0.811921         0.813923        0.003057               23   \n",
       "30           0.813100         0.813805        0.001399               24   \n",
       "49           0.813100         0.813418        0.002140               25   \n",
       "31           0.813184         0.813401        0.002058               26   \n",
       "23           0.813184         0.813401        0.002058               26   \n",
       "3            0.811163         0.813165        0.002822               28   \n",
       "17           0.811753         0.813047        0.002588               29   \n",
       "16           0.812931         0.813013        0.002147               30   \n",
       "8            0.812426         0.812896        0.002717               31   \n",
       "35           0.811079         0.812677        0.002416               32   \n",
       "20           0.810406         0.812374        0.002197               33   \n",
       "39           0.811500         0.812306        0.002258               34   \n",
       "22           0.811163         0.812088        0.002323               35   \n",
       "29           0.809816         0.812003        0.002197               36   \n",
       "4            0.811500         0.811953        0.001467               37   \n",
       "45           0.809901         0.811902        0.002178               38   \n",
       "27           0.809901         0.811566        0.002167               39   \n",
       "43           0.809648         0.810051        0.001929               40   \n",
       "41           0.807964         0.809966        0.003047               41   \n",
       "13           0.809816         0.809731        0.001811               42   \n",
       "14           0.804176         0.806061        0.002856               43   \n",
       "6            0.802576         0.805522        0.003627               44   \n",
       "24           0.803418         0.805488        0.002769               45   \n",
       "40           0.772942         0.765589        0.004709               46   \n",
       "15           0.771931         0.765168        0.004321               47   \n",
       "34           0.770163         0.763872        0.004047               48   \n",
       "47           0.769406         0.763300        0.003618               49   \n",
       "0            0.769995         0.762997        0.004576               50   \n",
       "\n",
       "    split0_train_score  split1_train_score  split2_train_score  \\\n",
       "11            0.951893            0.951598            0.952041   \n",
       "33            0.951893            0.951598            0.952041   \n",
       "9             0.951851            0.951619            0.951999   \n",
       "42            0.981481            0.981902            0.981860   \n",
       "5             0.954671            0.954797            0.955471   \n",
       "18            0.950946            0.950525            0.951157   \n",
       "19            0.982070            0.982239            0.982029   \n",
       "48            0.982028            0.982239            0.982050   \n",
       "7             0.953976            0.953598            0.954524   \n",
       "36            0.935184            0.936299            0.935438   \n",
       "12            0.965319            0.965530            0.966014   \n",
       "37            0.950525            0.950462            0.950758   \n",
       "21            0.950525            0.950462            0.950758   \n",
       "32            0.935121            0.935815            0.935795   \n",
       "28            0.963215            0.963720            0.964794   \n",
       "1             0.963215            0.963720            0.964794   \n",
       "2             0.954755            0.954608            0.955408   \n",
       "46            0.934847            0.936552            0.935711   \n",
       "10            0.983922            0.984680            0.984848   \n",
       "38            0.972137            0.972537            0.971907   \n",
       "26            0.972137            0.972537            0.971907   \n",
       "44            1.000000            1.000000            1.000000   \n",
       "25            0.969065            0.968939            0.967172   \n",
       "30            0.931438            0.931627            0.930997   \n",
       "49            0.994213            0.994423            0.994529   \n",
       "31            0.994150            0.994381            0.994592   \n",
       "23            0.994150            0.994381            0.994592   \n",
       "3             1.000000            1.000000            1.000000   \n",
       "17            0.997538            0.997853            0.997811   \n",
       "16            0.994150            0.994192            0.994360   \n",
       "8             0.997622            0.997727            0.997896   \n",
       "35            0.997601            0.997601            0.997811   \n",
       "20            0.908163            0.909173            0.908123   \n",
       "39            0.997643            0.997790            0.997917   \n",
       "22            0.908205            0.908458            0.908018   \n",
       "29            0.901513            0.902123            0.902104   \n",
       "4             0.899619            0.901576            0.900905   \n",
       "45            0.901534            0.902292            0.902378   \n",
       "27            0.901702            0.901934            0.901494   \n",
       "43            0.892190            0.893706            0.891877   \n",
       "41            0.999684            0.999726            0.999790   \n",
       "13            0.887982            0.888887            0.889057   \n",
       "14            1.000000            1.000000            1.000000   \n",
       "6             1.000000            1.000000            1.000000   \n",
       "24            1.000000            1.000000            1.000000   \n",
       "40            0.784697            0.785707            0.787016   \n",
       "15            0.782550            0.783729            0.784870   \n",
       "34            0.781224            0.783371            0.784638   \n",
       "47            0.778425            0.780130            0.780703   \n",
       "0             0.779036            0.779835            0.781271   \n",
       "\n",
       "    split3_train_score  split4_train_score  mean_train_score  std_train_score  \n",
       "11            0.950526            0.951707          0.951553         0.000536  \n",
       "33            0.950526            0.951707          0.951553         0.000536  \n",
       "9             0.950442            0.951917          0.951566         0.000576  \n",
       "42            0.981566            0.981524          0.981667         0.000178  \n",
       "5             0.954482            0.954926          0.954870         0.000335  \n",
       "18            0.950021            0.950928          0.950715         0.000403  \n",
       "19            0.981713            0.981924          0.981995         0.000174  \n",
       "48            0.981734            0.981924          0.981995         0.000165  \n",
       "7             0.953556            0.954105          0.953952         0.000356  \n",
       "36            0.934933            0.935461          0.935463         0.000460  \n",
       "12            0.964141            0.965279          0.965257         0.000616  \n",
       "37            0.949790            0.950528          0.950412         0.000327  \n",
       "21            0.949790            0.950528          0.950412         0.000327  \n",
       "32            0.934470            0.935630          0.935366         0.000514  \n",
       "28            0.963131            0.962943          0.963561         0.000668  \n",
       "1             0.963131            0.962943          0.963561         0.000668  \n",
       "2             0.954377            0.955031          0.954836         0.000356  \n",
       "46            0.935101            0.935419          0.935526         0.000590  \n",
       "10            0.983817            0.984512          0.984356         0.000412  \n",
       "38            0.971633            0.973276          0.972298         0.000572  \n",
       "26            0.971633            0.973276          0.972298         0.000572  \n",
       "44            1.000000            1.000000          1.000000         0.000000  \n",
       "25            0.968497            0.969277          0.968590         0.000754  \n",
       "30            0.930787            0.932873          0.931545         0.000729  \n",
       "49            0.994423            0.994297          0.994377         0.000110  \n",
       "31            0.994360            0.994297          0.994356         0.000143  \n",
       "23            0.994360            0.994297          0.994356         0.000143  \n",
       "3             1.000000            1.000000          1.000000         0.000000  \n",
       "17            0.997580            0.997601          0.997677         0.000129  \n",
       "16            0.994276            0.994045          0.994205         0.000108  \n",
       "8             0.997538            0.997622          0.997681         0.000123  \n",
       "35            0.997664            0.997433          0.997622         0.000122  \n",
       "20            0.908628            0.908905          0.908598         0.000410  \n",
       "39            0.997601            0.997538          0.997698         0.000137  \n",
       "22            0.908586            0.908842          0.908422         0.000288  \n",
       "29            0.901747            0.902087          0.901915         0.000245  \n",
       "4             0.900715            0.901582          0.900880         0.000721  \n",
       "45            0.902189            0.902298          0.902138         0.000308  \n",
       "27            0.901747            0.901582          0.901692         0.000150  \n",
       "43            0.893834            0.894175          0.893157         0.000935  \n",
       "41            0.999811            0.999642          0.999731         0.000063  \n",
       "13            0.889141            0.890303          0.889074         0.000741  \n",
       "14            1.000000            1.000000          1.000000         0.000000  \n",
       "6             1.000000            1.000000          1.000000         0.000000  \n",
       "24            1.000000            1.000000          1.000000         0.000000  \n",
       "40            0.783880            0.785762          0.785412         0.001062  \n",
       "15            0.782681            0.783805          0.783527         0.000847  \n",
       "34            0.781019            0.783406          0.782731         0.001393  \n",
       "47            0.778914            0.780754          0.779785         0.000949  \n",
       "0             0.779566            0.780312          0.780004         0.000756  "
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resultsRF = pd.DataFrame(searchRF.cv_results_)\n",
    "resultsRF.sort_values(by='rank_test_score')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Model Plus Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = RandomForestClassifier(max_depth=None, n_estimators=320, n_jobs=-1, random_state=42)\n",
    "score = cross_val_score(best_model, X_test1, y_train_target, scoring='accuracy', cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8098149237465961"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model.fit(X_test1, y_train_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x1a2136a358>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x1440 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "figsize = (5,20)\n",
    "\n",
    "name = 'Random Forest Classifier'\n",
    "importances = pd.Series(best_model.feature_importances_, X_test1.columns)\n",
    "title = f'{name}, max_depth={best_model.max_depth}'\n",
    "\n",
    "plt.figure(figsize=figsize)\n",
    "importances.sort_values().plot.barh(color='grey', title=title)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting package metadata: done\n",
      "Solving environment: \\ \n",
      "Warning: 2 possible package resolutions (only showing differing packages):\n",
      "  - anaconda::ca-certificates-2019.1.23-0\n",
      "  - defaults::ca-certificates-2019.1.23done\n",
      "\n",
      "## Package Plan ##\n",
      "\n",
      "  environment location: /Users/lambda_school_loaner_95/anaconda3\n",
      "\n",
      "  added / updated specs:\n",
      "    - eli5\n",
      "\n",
      "\n",
      "The following packages will be downloaded:\n",
      "\n",
      "    package                    |            build\n",
      "    ---------------------------|-----------------\n",
      "    cairo-1.14.12              |    h9d4d9ac_1005         1.3 MB  conda-forge\n",
      "    certifi-2019.3.9           |           py37_0         149 KB  conda-forge\n",
      "    conda-4.6.14               |           py37_0         2.1 MB  conda-forge\n",
      "    eli5-0.8.1                 |             py_0          65 KB  conda-forge\n",
      "    fontconfig-2.13.1          |    h1027ab8_1000         269 KB  conda-forge\n",
      "    fribidi-1.0.5              |    h1de35cc_1000          62 KB  conda-forge\n",
      "    graphite2-1.3.13           |    h2098e52_1000          84 KB  conda-forge\n",
      "    graphviz-2.40.1            |       hefbbd9a_2         6.7 MB\n",
      "    harfbuzz-1.9.0             |    h9889186_1001         769 KB  conda-forge\n",
      "    openssl-1.1.1b             |       h1de35cc_1         3.5 MB  conda-forge\n",
      "    pango-1.42.4               |       h060686c_0         523 KB\n",
      "    pixman-0.34.0              |    h1de35cc_1003         597 KB  conda-forge\n",
      "    python-graphviz-0.10.1     |             py_0          17 KB  conda-forge\n",
      "    tabulate-0.8.3             |             py_0          23 KB  conda-forge\n",
      "    typing-3.6.4               |           py37_0          45 KB\n",
      "    ------------------------------------------------------------\n",
      "                                           Total:        16.1 MB\n",
      "\n",
      "The following NEW packages will be INSTALLED:\n",
      "\n",
      "  cairo              conda-forge/osx-64::cairo-1.14.12-h9d4d9ac_1005\n",
      "  eli5               conda-forge/noarch::eli5-0.8.1-py_0\n",
      "  fontconfig         conda-forge/osx-64::fontconfig-2.13.1-h1027ab8_1000\n",
      "  fribidi            conda-forge/osx-64::fribidi-1.0.5-h1de35cc_1000\n",
      "  graphite2          conda-forge/osx-64::graphite2-1.3.13-h2098e52_1000\n",
      "  graphviz           pkgs/main/osx-64::graphviz-2.40.1-hefbbd9a_2\n",
      "  harfbuzz           conda-forge/osx-64::harfbuzz-1.9.0-h9889186_1001\n",
      "  pango              pkgs/main/osx-64::pango-1.42.4-h060686c_0\n",
      "  pixman             conda-forge/osx-64::pixman-0.34.0-h1de35cc_1003\n",
      "  python-graphviz    conda-forge/noarch::python-graphviz-0.10.1-py_0\n",
      "  tabulate           conda-forge/noarch::tabulate-0.8.3-py_0\n",
      "  typing             pkgs/main/osx-64::typing-3.6.4-py37_0\n",
      "\n",
      "The following packages will be SUPERSEDED by a higher-priority channel:\n",
      "\n",
      "  certifi                                          anaconda --> conda-forge\n",
      "  conda                                            anaconda --> conda-forge\n",
      "  openssl                                          anaconda --> conda-forge\n",
      "\n",
      "\n",
      "\n",
      "Downloading and Extracting Packages\n",
      "openssl-1.1.1b       | 3.5 MB    | ##################################### | 100% \n",
      "fribidi-1.0.5        | 62 KB     | ##################################### | 100% \n",
      "certifi-2019.3.9     | 149 KB    | ##################################### | 100% \n",
      "tabulate-0.8.3       | 23 KB     | ##################################### | 100% \n",
      "conda-4.6.14         | 2.1 MB    | ##################################### | 100% \n",
      "python-graphviz-0.10 | 17 KB     | ##################################### | 100% \n",
      "pixman-0.34.0        | 597 KB    | ##################################### | 100% \n",
      "eli5-0.8.1           | 65 KB     | ##################################### | 100% \n",
      "typing-3.6.4         | 45 KB     | ##################################### | 100% \n",
      "harfbuzz-1.9.0       | 769 KB    | ##################################### | 100% \n",
      "cairo-1.14.12        | 1.3 MB    | ##################################### | 100% \n",
      "graphite2-1.3.13     | 84 KB     | ##################################### | 100% \n",
      "graphviz-2.40.1      | 6.7 MB    | ##################################### | 100% \n",
      "pango-1.42.4         | 523 KB    | ##################################### | 100% \n",
      "fontconfig-2.13.1    | 269 KB    | ##################################### | 100% \n",
      "Preparing transaction: done\n",
      "Verifying transaction: done\n",
      "Executing transaction: done\n",
      "\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "conda install -c conda-forge eli5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PermutationImportance(cv='prefit',\n",
       "           estimator=RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=320, n_jobs=-1,\n",
       "            oob_score=False, random_state=42, verbose=0, warm_start=False),\n",
       "           n_iter=2, random_state=42, refit=True, scoring='accuracy')"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import eli5\n",
    "from eli5.sklearn import PermutationImportance\n",
    "\n",
    "permuter = PermutationImportance(best_model, scoring='accuracy', cv='prefit',\n",
    "                                n_iter=2, random_state=42)\n",
    "\n",
    "permuter.fit(X_test1.values, y_train_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <style>\n",
       "    table.eli5-weights tr:hover {\n",
       "        filter: brightness(85%);\n",
       "    }\n",
       "</style>\n",
       "\n",
       "\n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "        <table class=\"eli5-weights eli5-feature-importances\" style=\"border-collapse: collapse; border: none; margin-top: 0em; table-layout: auto;\">\n",
       "    <thead>\n",
       "    <tr style=\"border: none;\">\n",
       "        <th style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">Weight</th>\n",
       "        <th style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">Feature</th>\n",
       "    </tr>\n",
       "    </thead>\n",
       "    <tbody>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(120, 100.00%, 80.00%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                0.1608\n",
       "                \n",
       "                    &plusmn; 0.0011\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                quantity\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(120, 100.00%, 91.07%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                0.0508\n",
       "                \n",
       "                    &plusmn; 0.0000\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                extraction_type\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(120, 100.00%, 91.42%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                0.0480\n",
       "                \n",
       "                    &plusmn; 0.0005\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                waterpoint_type\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(120, 100.00%, 92.99%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                0.0360\n",
       "                \n",
       "                    &plusmn; 0.0002\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                years_service\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(120, 100.00%, 93.09%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                0.0352\n",
       "                \n",
       "                    &plusmn; 0.0009\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                amount_tsh\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(120, 100.00%, 96.52%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                0.0132\n",
       "                \n",
       "                    &plusmn; 0.0007\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                population\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(120, 100.00%, 96.73%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                0.0121\n",
       "                \n",
       "                    &plusmn; 0.0000\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                id\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(120, 100.00%, 97.12%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                0.0101\n",
       "                \n",
       "                    &plusmn; 0.0008\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                payment_type\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(120, 100.00%, 97.31%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                0.0091\n",
       "                \n",
       "                    &plusmn; 0.0003\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                funder\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(120, 100.00%, 97.69%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                0.0074\n",
       "                \n",
       "                    &plusmn; 0.0003\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                offset_days\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(120, 100.00%, 97.84%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                0.0067\n",
       "                \n",
       "                    &plusmn; 0.0001\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                gps_height\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(120, 100.00%, 98.00%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                0.0060\n",
       "                \n",
       "                    &plusmn; 0.0002\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                management\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(120, 100.00%, 98.06%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                0.0058\n",
       "                \n",
       "                    &plusmn; 0.0001\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                source\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(120, 100.00%, 98.39%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                0.0044\n",
       "                \n",
       "                    &plusmn; 0.0002\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                latitude\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(120, 100.00%, 98.49%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                0.0040\n",
       "                \n",
       "                    &plusmn; 0.0000\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                installer\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(120, 100.00%, 98.50%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                0.0040\n",
       "                \n",
       "                    &plusmn; 0.0001\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                radial_r\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(120, 100.00%, 98.54%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                0.0038\n",
       "                \n",
       "                    &plusmn; 0.0001\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                longitude\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(120, 100.00%, 98.65%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                0.0034\n",
       "                \n",
       "                    &plusmn; 0.0001\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                rot60Y\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(120, 100.00%, 98.66%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                0.0034\n",
       "                \n",
       "                    &plusmn; 0.0000\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                rot45Y\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(120, 100.00%, 98.70%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                0.0032\n",
       "                \n",
       "                    &plusmn; 0.0001\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                rot60X\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(120, 100.00%, 98.76%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                0.0030\n",
       "                \n",
       "                    &plusmn; 0.0001\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                rot30X\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(120, 100.00%, 98.77%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                0.0030\n",
       "                \n",
       "                    &plusmn; 0.0004\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                rot30Y\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(120, 100.00%, 98.85%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                0.0027\n",
       "                \n",
       "                    &plusmn; 0.0002\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                rot45X\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(120, 100.00%, 98.96%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                0.0023\n",
       "                \n",
       "                    &plusmn; 0.0003\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                lga\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(120, 100.00%, 99.07%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                0.0020\n",
       "                \n",
       "                    &plusmn; 0.0002\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                quality_group\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(120, 100.00%, 99.11%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                0.0019\n",
       "                \n",
       "                    &plusmn; 0.0001\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                scheme_management\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(120, 100.00%, 99.18%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                0.0017\n",
       "                \n",
       "                    &plusmn; 0.0002\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                month_recorded\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(120, 100.00%, 99.61%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                0.0006\n",
       "                \n",
       "                    &plusmn; 0.0000\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                region\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(120, 100.00%, 99.81%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                0.0002\n",
       "                \n",
       "                    &plusmn; 0.0001\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                district_code\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(120, 100.00%, 99.85%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                0.0001\n",
       "                \n",
       "                    &plusmn; 0.0000\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                basin\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(120, 100.00%, 99.91%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                0.0001\n",
       "                \n",
       "                    &plusmn; 0.0000\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                permit\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "    \n",
       "    </tbody>\n",
       "</table>\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_names = X_test1.columns.tolist()\n",
    "eli5.show_weights(permuter, top=None, feature_names=feature_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Realizations\n",
    "I hope to be doing further exploration on this dataset.  Biggest take aways so far\n",
    "- Having a clear process is very helpful to make quick incrememntal gains\n",
    "- Going for an over fitted model early on was helpful mindset to have\n",
    "- Domain knowledge is best for efficient feature engineering\n",
    "- Goal should be to plateau on score as fast as possible, then analyze the data to form real conclusions (the extra .0001 percent is not worth the time)\n",
    "- don't lose track of the problem at hand when trying to make 'the best' model\n",
    "- Stacking models together can be very valuable\n",
    "- simple is good"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submission blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# estimator = RandomForestClassifier(max_depth=None, n_estimators=320, n_jobs=-1, random_state=42)\n",
    "# estimator.fit(X_test1, y_train_target)\n",
    "\n",
    "# y_pred = estimator.predict(X_teste2)\n",
    "\n",
    "# sample_submission = pd.read_csv('https://storage.googleapis.com/kaggle-competitions-data/kaggle/14688/453539/sample_submission.csv?GoogleAccessId=web-data@kaggle-161607.iam.gserviceaccount.com&Expires=1558636843&Signature=F8VvtSyMe9kPksSP5FK2hKnYJw6XMBytCjBIg%2Fkec7Ddcf%2Bue4Ge%2FGxHKWkr%2FZBgR6i%2FZt36WV4cSDnD2XCSJJ4%2F%2Bk23YsN9fi7F29W4E6worpqUlfCTk%2FNNB3y96EIkTZ7YNzh7inKZ93tB%2BxcAkyKnOseWQ3y8iGRDPRU7%2BXPeYretRM%2FBLqSbYU4gRUGxhSCwww3cRbsWi%2FRMcWq5YyypMOCuCBI7hJ7HIUc47u2WAjsleNEKGvJ69I82aral8%2FzercReL2rGCKfTTPsJ0WRH%2FTdJjHQ2O6EJ4l9AfUe7Dqjqjj8XCYFKaFfec5B8htoxTgkBcqG4GKhWEU%2B65w%3D%3D')\n",
    "# baseline_submission13 = sample_submission.copy()\n",
    "# baseline_submission13['status_group'] = y_pred\n",
    "# baseline_submission13.to_csv('baseline_submission13.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "#best_estimator = searchRF.best_estimator_\n",
    "# model.fit(X_test1, y_train_target)\n",
    "\n",
    "# y_pred = model.predict(X_teste2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample_submission = pd.read_csv('https://storage.googleapis.com/kaggle-competitions-data/kaggle/14688/453539/sample_submission.csv?GoogleAccessId=web-data@kaggle-161607.iam.gserviceaccount.com&Expires=1558905720&Signature=INefpYzy5J4FBdIujvu9QVAhAMmJ0ahuW3bootCsHUaUuuKoJzgyA5rAsknVCOFY8hSWDQrZ%2FgBkCgnLniVB6rHKeMNEl4HAYOe0RN54uY0bvqmYKWr3aARaQk7YCKka28S5WtjI7HkjM%2BbXZkP4dYlt8NRjAoVSdiaWsysUwxXeRRGIBuhrt%2FuBSAPk3CegyHXikAsEHx0VKhdRNIv%2BrHyXOu2Y1ldloSvxcrQdmI%2BirQrxXJLT7XD6tI5leaYbXit6tQ2X5syfBek31o31B7AkMqz%2B9zIlZ5SBDMhOXbdaxEZMGaE7NRFEioLq%2Fp3GtolWe97qrOnvoHybd2PBsg%3D%3D')\n",
    "# baseline_submission16 = sample_submission.copy()\n",
    "# baseline_submission16['status_group'] = y_pred\n",
    "# baseline_submission16.to_csv('baseline_submission16.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Filenames of your submissions you want to ensemble\n",
    "# sample_submission = pd.read_csv('https://storage.googleapis.com/kaggle-competitions-data/kaggle/14688/453539/sample_submission.csv?GoogleAccessId=web-data@kaggle-161607.iam.gserviceaccount.com&Expires=1558636843&Signature=F8VvtSyMe9kPksSP5FK2hKnYJw6XMBytCjBIg%2Fkec7Ddcf%2Bue4Ge%2FGxHKWkr%2FZBgR6i%2FZt36WV4cSDnD2XCSJJ4%2F%2Bk23YsN9fi7F29W4E6worpqUlfCTk%2FNNB3y96EIkTZ7YNzh7inKZ93tB%2BxcAkyKnOseWQ3y8iGRDPRU7%2BXPeYretRM%2FBLqSbYU4gRUGxhSCwww3cRbsWi%2FRMcWq5YyypMOCuCBI7hJ7HIUc47u2WAjsleNEKGvJ69I82aral8%2FzercReL2rGCKfTTPsJ0WRH%2FTdJjHQ2O6EJ4l9AfUe7Dqjqjj8XCYFKaFfec5B8htoxTgkBcqG4GKhWEU%2B65w%3D%3D')\n",
    "# files = ['baseline_submission15.csv', 'baseline_submission14.csv', 'baseline_submission13.csv',\n",
    "#         'baseline_submission5.csv', 'baseline_submission4.csv', 'baseline_submission16.csv']\n",
    "\n",
    "# submissions = (pd.read_csv(file)[['status_group']] for file in files)\n",
    "# ensemble = pd.concat(submissions, axis='columns')\n",
    "# majority_vote = ensemble.mode(axis='columns')[0]\n",
    "\n",
    "\n",
    "# sample_submission = pd.read_csv('https://storage.googleapis.com/kaggle-competitions-data/kaggle/14688/453539/sample_submission.csv?GoogleAccessId=web-data@kaggle-161607.iam.gserviceaccount.com&Expires=1558905720&Signature=INefpYzy5J4FBdIujvu9QVAhAMmJ0ahuW3bootCsHUaUuuKoJzgyA5rAsknVCOFY8hSWDQrZ%2FgBkCgnLniVB6rHKeMNEl4HAYOe0RN54uY0bvqmYKWr3aARaQk7YCKka28S5WtjI7HkjM%2BbXZkP4dYlt8NRjAoVSdiaWsysUwxXeRRGIBuhrt%2FuBSAPk3CegyHXikAsEHx0VKhdRNIv%2BrHyXOu2Y1ldloSvxcrQdmI%2BirQrxXJLT7XD6tI5leaYbXit6tQ2X5syfBek31o31B7AkMqz%2B9zIlZ5SBDMhOXbdaxEZMGaE7NRFEioLq%2Fp3GtolWe97qrOnvoHybd2PBsg%3D%3D')\n",
    "# submission = sample_submission.copy()\n",
    "# submission['status_group'] = majority_vote\n",
    "# submission.to_csv('ensemble-submission2.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
